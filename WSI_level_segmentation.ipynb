{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ee9694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import openslide\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import timm\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torchmetrics\n",
    "from torch.nn.modules.batchnorm import _BatchNorm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import pyvips\n",
    "import json\n",
    "import cv2\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d283796",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list={\n",
    "    'background':0,\n",
    "    'Tumor':1,\n",
    "    'Stroma':2,\n",
    "    'Immune cells':3,\n",
    "    'Necrosis':4,\n",
    "    'alveoli':5,\n",
    "    'Other':6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "249c7651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    \"\"\"Feature extoractor block\"\"\"\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        cnn1= timm.create_model('tf_efficientnetv2_s', pretrained=True)\n",
    "        self.feature_ex = nn.Sequential(*list(cnn1.children())[:-1])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        features = self.feature_ex(inputs)\n",
    "        \n",
    "        return features\n",
    "class custom_model(nn.Module):\n",
    "    def __init__(self, num_classes, image_feature_dim,feature_extractor_scale1: FeatureExtractor):\n",
    "        super(custom_model, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.image_feature_dim = image_feature_dim\n",
    "\n",
    "        # Remove the classification head of the CNN model\n",
    "        self.feature_extractor = feature_extractor_scale1\n",
    "        # Classification layer\n",
    "        self.classification_layer = nn.Linear(image_feature_dim, num_classes)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        batch_size, channels, height, width = inputs.size()\n",
    "        \n",
    "        # Feature extraction using the pre-trained CNN\n",
    "        features = self.feature_extractor(inputs)  # Shape: (batch_size, 2048, 1, 1)\n",
    "        \n",
    "        # Classification layer\n",
    "        logits = self.classification_layer(features)  # Shape: (batch_size, num_classes)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "\n",
    "Feature_Extractor=FeatureExtractor()\n",
    "model = custom_model(len(class_list),1280,Feature_Extractor)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('../../model/IHC_Tissue_Region_classification/check.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b97aa98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b3c45fbf3648f8b49a18cf15a104ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored from cffi callback <function _log_handler_callback at 0x7fd4be197e20>:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/work/.local/lib/python3.12/site-packages/pyvips/__init__.py\", line 149, in _log_handler_callback\n",
      "    def _log_handler_callback(domain, level, message, user_data):\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "slide_path=glob('../../data/WSI_test/BR/*KI*.tiff')\n",
    "image_size=256\n",
    "image_resize=128\n",
    "label_img_resize=8 #패치 하나당 리사이즈 크기\n",
    "count=0\n",
    "pixel_size=0.25 #mpp\n",
    "pixel_extend=pixel_size*pixel_size*image_size*image_size\n",
    "\n",
    "# 색상 매핑 사전 정의\n",
    "# color_map = {\n",
    "#     0: [255, 255, 255],  # background - white\n",
    "#     1: [255, 0, 0],      # Tumor - red\n",
    "#     2: [0, 255, 0],      # Stroma - green\n",
    "#     3: [0, 0, 255],      # Immune cells - blue\n",
    "#     4: [255, 255, 0],    # Necrosis - yellow\n",
    "#     5: [0, 255, 255],    # alveoli - cyan\n",
    "#     6: [128, 128, 128]   # Other - gray\n",
    "# }\n",
    "color_map = {\n",
    "    0: [255, 255, 255],  # background - white\n",
    "    1: [255, 0, 0],      # Tumor - red\n",
    "    2: [0, 255, 0],      # Stroma - green\n",
    "    3: [0, 0, 255],      # Immune cells - blue\n",
    "    4: [255, 255, 255],    # Necrosis - yellow\n",
    "    5: [0, 255, 255],    # alveoli -alveoli\n",
    "    6: [255, 255, 255]   # Other - gray\n",
    "}\n",
    "label_size = {\n",
    "    'total': 0, #total area\n",
    "    'Tumor': 0,      # Tumor\n",
    "    'Stroma': 0,      # Stroma \n",
    "    'Immune cells': 0,  # Immune cells\n",
    "    'alveoli': 0,    # alveoli\n",
    "}\n",
    "\n",
    "for i in tqdm(range(len(slide_path))):\n",
    "\n",
    "    temp_patch_size=2048\n",
    "    temp_patch_count=temp_patch_size//image_size\n",
    "    file_name=os.path.basename(slide_path[i]).split('.')[0]\n",
    "    slide = pyvips.Image.new_from_file(slide_path[i])\n",
    "    trans = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ])\n",
    "    temp_img_list=[]\n",
    "    for row in range(0,slide.height//temp_patch_size):\n",
    "        temp_col_list=[]\n",
    "        for col in range(0,slide.width//temp_patch_size):\n",
    "            patch = slide.crop(col*temp_patch_size, row*temp_patch_size, temp_patch_size, temp_patch_size)\n",
    "            patch = np.ndarray(buffer=patch.write_to_memory(),\n",
    "                                dtype=np.uint8,\n",
    "                                shape=[patch.height, patch.width, patch.bands])\n",
    "            temp_col_list.append(patch)\n",
    "        temp_img_list.append(temp_col_list)\n",
    "\n",
    "\n",
    "    # 병렬 처리를 위한 함수 정의\n",
    "    def process_large_patch(patch_data):\n",
    "        row, col, img = patch_data\n",
    "        \n",
    "        # 이미지를 텐서로 변환\n",
    "        img_tensor = trans(img[:,:,:3]).to(device)  # (C, H, W)\n",
    "        \n",
    "        # 작은 패치들로 분할하여 배치 생성\n",
    "        patches = []\n",
    "        positions = []\n",
    "        \n",
    "        for detail_row in range(temp_patch_count):\n",
    "            for detail_col in range(temp_patch_count):\n",
    "                patch = img_tensor[:, detail_row*image_size:(detail_row+1)*image_size, \n",
    "                                detail_col*image_size:(detail_col+1)*image_size]\n",
    "                patch = F.interpolate(patch.unsqueeze(0), size=(image_resize, image_resize), mode='bilinear', align_corners=False).squeeze(0)\n",
    "                patches.append(patch)\n",
    "                positions.append((detail_row, detail_col))\n",
    "        \n",
    "        # 배치로 변환\n",
    "        patches_batch = torch.stack(patches)  # (N, C, H, W)\n",
    "        \n",
    "        # 배치 예측 (1024x1024 패치에서 8x8=64개의 128x128 패치가 나오므로)\n",
    "        # 한 번에 모든 패치를 처리할 수 있음 (배치 크기 = 64)\n",
    "        predictions = []\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # patches_batch는 정확히 64개 (8x8)의 패치를 포함\n",
    "            outputs = model(patches_batch)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predictions = preds.cpu().numpy()\n",
    "        \n",
    "        return row, col, predictions, positions\n",
    "\n",
    "    # 병렬 처리를 위한 데이터 준비\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    import multiprocessing as mp\n",
    "\n",
    "    # 레이블 이미지 초기화\n",
    "    label_img = np.zeros((label_img_resize*temp_patch_count*len(temp_img_list),\n",
    "                        label_img_resize*temp_patch_count*len(temp_img_list[0]), 3), dtype=np.uint8)\n",
    "\n",
    "    # 패치 데이터 준비\n",
    "    patch_data_list = []\n",
    "    for row in range(len(temp_img_list)):\n",
    "        for col in range(len(temp_img_list[0])):\n",
    "            patch_data_list.append((row, col, temp_img_list[row][col]))\n",
    "\n",
    "    print(f\"총 {len(patch_data_list)}개의 큰 패치를 처리합니다...\")\n",
    "\n",
    "    # 순차적으로 처리 (GPU 메모리 관리를 위해)\n",
    "    for patch_data in tqdm(patch_data_list, desc=\"Processing patches\"):\n",
    "        row, col, predictions, positions = process_large_patch(patch_data)\n",
    "        \n",
    "        # 예측 결과를 레이블 이미지에 적용\n",
    "        for pred, (detail_row, detail_col) in zip(predictions, positions):\n",
    "            y_start = row * temp_patch_count * label_img_resize + detail_row * label_img_resize\n",
    "            y_end = y_start + label_img_resize\n",
    "            x_start = col * temp_patch_count * label_img_resize + detail_col * label_img_resize\n",
    "            x_end = x_start + label_img_resize\n",
    "            if pred!=0:\n",
    "                label_size['total']+=pixel_extend\n",
    "            label_img[y_start:y_end, x_start:x_end] = color_map[pred]\n",
    "            \n",
    "            if pred==1:\n",
    "                label_size['Tumor']+=pixel_extend\n",
    "            elif pred==2:\n",
    "                label_size['Stroma']+=pixel_extend\n",
    "            elif pred==3:\n",
    "                label_size['Immune cells']+=pixel_extend\n",
    "            # elif pred==4:\n",
    "            #     label_size['Necrosis']+=pixel_extend\n",
    "            elif pred==5:\n",
    "                label_size['alveoli']+=pixel_extend\n",
    "\n",
    "    print(\"세그멘테이션 완료. 결과를 저장합니다...\")\n",
    "    def create_dir(path):\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "    create_dir('../../results/BR/WSI_classification/')\n",
    "    Image.fromarray(label_img).save(f'../../results/BR/WSI_classification/{file_name}_segmentation.png')\n",
    "    json.dump(label_size, open(f'../../results/BR/WSI_classification/{file_name}_segmentation_size.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42767be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_patch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d50a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "slide_path[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
